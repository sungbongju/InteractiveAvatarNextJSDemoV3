import {
  AvatarQuality,
  StreamingEvents,
  VoiceChatTransport,
  VoiceEmotion,
  StartAvatarRequest,
  STTProvider,
  ElevenLabsModel,
  TaskType,
} from "@heygen/streaming-avatar";
import { useEffect, useRef, useState } from "react";
import { useMemoizedFn, useUnmount } from "ahooks";

import { useStreamingAvatarSession } from "./logic/useStreamingAvatarSession";
import { StreamingAvatarProvider, StreamingAvatarSessionState } from "./logic";

import { AVATARS } from "@/app/lib/constants";

const DEFAULT_CONFIG: StartAvatarRequest = {
  quality: AvatarQuality.Low,
  avatarName: AVATARS[0].avatar_id,
  voice: {
    rate: 1.5,
    emotion: VoiceEmotion.EXCITED,
    model: ElevenLabsModel.eleven_flash_v2_5,
  },
  language: "ko",
  voiceChatTransport: VoiceChatTransport.WEBSOCKET,
  sttSettings: {
    provider: STTProvider.DEEPGRAM,
  },
};

interface ChatMessage {
  role: "user" | "assistant";
  content: string;
}

// VAD ì„¤ì •
const VAD_CONFIG = {
  SILENCE_THRESHOLD: 0.01,      // ì¹¨ë¬µ íŒë‹¨ ê¸°ì¤€ (ë‚®ì„ìˆ˜ë¡ ë¯¼ê°)
  SPEECH_THRESHOLD: 0.02,       // ë§í•˜ê¸° ì‹œì‘ íŒë‹¨ ê¸°ì¤€
  SILENCE_DURATION: 1500,       // ì¹¨ë¬µ ì§€ì† ì‹œê°„ (ms) í›„ ë…¹ìŒ ì¤‘ì§€
  MIN_RECORDING_TIME: 500,      // ìµœì†Œ ë…¹ìŒ ì‹œê°„ (ms)
};

function InteractiveAvatar() {
  const {
    initAvatar,
    startAvatar,
    stopAvatar,
    sessionState,
    stream,
    avatarRef,
  } = useStreamingAvatarSession();

  const [config] = useState<StartAvatarRequest>(DEFAULT_CONFIG);
  const [inputText, setInputText] = useState("");
  const [isLoading, setIsLoading] = useState(false);
  const [chatHistory, setChatHistory] = useState<ChatMessage[]>([]);
  const [isListening, setIsListening] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [micEnabled, setMicEnabled] = useState(false);
  const mediaStream = useRef<HTMLVideoElement>(null);
  const isProcessingRef = useRef(false);
  const hasGreetedRef = useRef(false);
  const hasStartedRef = useRef(false);
  const userNameRef = useRef<string>('');
  const userStatsRef = useRef<any>(null);
  const isAvatarTalkingRef = useRef(false);  // ğŸ†• ì•„ë°”íƒ€ ë§í•˜ëŠ” ì¤‘ ì²´í¬
  
  // ğŸ†• Whisper STT + VAD ê´€ë ¨
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const micStreamRef = useRef<MediaStream | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const vadIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const silenceStartRef = useRef<number | null>(null);
  const recordingStartRef = useRef<number | null>(null);
  const isVadActiveRef = useRef(false);
  const isRecordingRef = useRef(false);  // ğŸ†• ë™ê¸° ì²´í¬ìš©

  async function fetchAccessToken() {
    try {
      const response = await fetch("/api/get-access-token", {
        method: "POST",
      });
      const token = await response.text();
      console.log("Access Token:", token);
      return token;
    } catch (error) {
      console.error("Error fetching access token:", error);
      throw error;
    }
  }

  // ============================================
  // ğŸ†• Whisper STT í•¨ìˆ˜
  // ============================================
  const transcribeWithWhisper = async (audioBlob: Blob): Promise<string> => {
    try {
      console.log("ğŸ¤ Whisperë¡œ ë³€í™˜ ì¤‘...", audioBlob.size, "bytes");
      
      const formData = new FormData();
      formData.append("audio", audioBlob, "recording.webm");

      const response = await fetch("/api/whisper", {
        method: "POST",
        body: formData,
      });

      const data = await response.json();
      
      if (data.error) {
        console.error("Whisper ì—ëŸ¬:", data.error);
        return "";
      }
      
      console.log("ğŸ¤ Whisper ê²°ê³¼:", data.text);
      return data.text || "";
    } catch (error) {
      console.error("Whisper API í˜¸ì¶œ ì‹¤íŒ¨:", error);
      return "";
    }
  };

  // ============================================
  // ğŸ†• VAD (Voice Activity Detection) í•¨ìˆ˜ë“¤
  // ============================================
  const getAudioLevel = (): number => {
    if (!analyserRef.current) return 0;
    
    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
    analyserRef.current.getByteFrequencyData(dataArray);
    
    // í‰ê·  ìŒëŸ‰ ê³„ì‚°
    const sum = dataArray.reduce((a, b) => a + b, 0);
    const average = sum / dataArray.length / 255; // 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
    
    return average;
  };

  const startVAD = () => {
    if (vadIntervalRef.current) return;
    
    isVadActiveRef.current = true;
    
    vadIntervalRef.current = setInterval(() => {
      // ì²˜ë¦¬ ì¤‘ì´ê±°ë‚˜ ì•„ë°”íƒ€ê°€ ë§í•˜ëŠ” ì¤‘ì´ë©´ ë¬´ì‹œ
      if (isProcessingRef.current || isLoading || isAvatarTalkingRef.current) {
        return;
      }

      const level = getAudioLevel();
      
      if (!isRecordingRef.current) {
        // ë…¹ìŒ ì¤‘ ì•„ë‹ ë•Œ: ìŒì„± ê°ì§€ë˜ë©´ ë…¹ìŒ ì‹œì‘
        if (level > VAD_CONFIG.SPEECH_THRESHOLD) {
          console.log("ğŸ¤ ìŒì„± ê°ì§€! ë…¹ìŒ ì‹œì‘", level.toFixed(3));
          startRecording();
        }
      } else {
        // ë…¹ìŒ ì¤‘ì¼ ë•Œ: ì¹¨ë¬µ ê°ì§€ë˜ë©´ ë…¹ìŒ ì¤‘ì§€
        if (level < VAD_CONFIG.SILENCE_THRESHOLD) {
          if (!silenceStartRef.current) {
            silenceStartRef.current = Date.now();
          } else {
            const silenceDuration = Date.now() - silenceStartRef.current;
            const recordingDuration = Date.now() - (recordingStartRef.current || 0);
            
            // ìµœì†Œ ë…¹ìŒ ì‹œê°„ ì´ìƒì´ê³ , ì¹¨ë¬µì´ ì¼ì • ì‹œê°„ ì§€ì†ë˜ë©´ ì¤‘ì§€
            if (recordingDuration > VAD_CONFIG.MIN_RECORDING_TIME && 
                silenceDuration > VAD_CONFIG.SILENCE_DURATION) {
              console.log("ğŸ”‡ ì¹¨ë¬µ ê°ì§€! ë…¹ìŒ ì¤‘ì§€");
              stopRecording();
            }
          }
        } else {
          // ì†Œë¦¬ê°€ ë‚˜ë©´ ì¹¨ë¬µ íƒ€ì´ë¨¸ ë¦¬ì…‹
          silenceStartRef.current = null;
        }
      }
    }, 100); // 100msë§ˆë‹¤ ì²´í¬
  };

  const stopVAD = () => {
    if (vadIntervalRef.current) {
      clearInterval(vadIntervalRef.current);
      vadIntervalRef.current = null;
    }
    isVadActiveRef.current = false;
  };

  const startRecording = () => {
    // ğŸ†• refë¡œ ë™ê¸° ì²´í¬
    if (!micStreamRef.current || isRecordingRef.current) return;
    
    isRecordingRef.current = true;  // ğŸ†• ì¦‰ì‹œ ì„¤ì •
    
    try {
      const mediaRecorder = new MediaRecorder(micStreamRef.current, {
        mimeType: "audio/webm",
      });
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];
      recordingStartRef.current = Date.now();
      silenceStartRef.current = null;

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: "audio/webm" });
        
        // ë„ˆë¬´ ì§§ì€ ë…¹ìŒ ë¬´ì‹œ
        if (audioBlob.size < 3000) {
          console.log("ë…¹ìŒì´ ë„ˆë¬´ ì§§ìŒ, ë¬´ì‹œ");
          isRecordingRef.current = false;
          setIsRecording(false);
          return;
        }

        // Whisperë¡œ í…ìŠ¤íŠ¸ ë³€í™˜
        const transcript = await transcribeWithWhisper(audioBlob);
        
        if (transcript && transcript.trim()) {
          await handleUserSpeech(transcript);
        }
        
        isRecordingRef.current = false;
        setIsRecording(false);
      };

      mediaRecorder.start();
      setIsRecording(true);
      setIsListening(true);
    } catch (error) {
      console.error("ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨:", error);
      isRecordingRef.current = false;
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
      mediaRecorderRef.current.stop();
    }
    isRecordingRef.current = false;  // ğŸ†• ì¦‰ì‹œ ì„¤ì •
    setIsListening(false);
    silenceStartRef.current = null;
  };

  // ============================================
  // ğŸ†• ë§ˆì´í¬ + VAD ì´ˆê¸°í™”
  // ============================================
  const initMicAndVAD = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      micStreamRef.current = stream;
      
      // AudioContext ì„¤ì •
      audioContextRef.current = new AudioContext();
      const source = audioContextRef.current.createMediaStreamSource(stream);
      analyserRef.current = audioContextRef.current.createAnalyser();
      analyserRef.current.fftSize = 256;
      source.connect(analyserRef.current);
      
      setMicEnabled(true);
      console.log("ğŸ¤ ë§ˆì´í¬ + VAD ì´ˆê¸°í™” ì™„ë£Œ!");
      
      // VAD ì‹œì‘
      startVAD();
      
    } catch (error) {
      console.error("ë§ˆì´í¬ ì´ˆê¸°í™” ì‹¤íŒ¨:", error);
    }
  };

  const cleanupMicAndVAD = () => {
    stopVAD();
    stopRecording();
    
    if (micStreamRef.current) {
      micStreamRef.current.getTracks().forEach(track => track.stop());
      micStreamRef.current = null;
    }
    
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }
    
    setMicEnabled(false);
  };

  // ============================================
  // API í˜¸ì¶œ í•¨ìˆ˜
  // ============================================
  const callChatAPI = async (
    type: "greeting" | "game_explain" | "chat",
    options: {
      message?: string;
      history?: ChatMessage[];
      game?: string;
    } = {}
  ) => {
    try {
      const response = await fetch("/api/chat", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          type: type,
          message: options.message || '',
          history: options.history || [],
          game: options.game || '',
          userName: userNameRef.current,
          userStats: userStatsRef.current,
        }),
      });
      const data = await response.json();
      return data.reply;
    } catch (error) {
      console.error("Chat API error:", error);
      return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.";
    }
  };

  const speakWithAvatar = async (text: string) => {
    console.log("=== Attempting to speak ===");
    console.log("Avatar ref exists:", !!avatarRef.current);
    console.log("Text to speak:", text);
    
    if (!avatarRef.current || !text) {
      console.log("Cannot speak - missing avatar or text");
      return;
    }
    
    try {
      console.log("Calling avatar.speak()...");
      await avatarRef.current.speak({
        text: text,
        taskType: TaskType.REPEAT,
      });
      console.log("Speak successful!");
    } catch (error) {
      console.error("Avatar speak error:", error);
    }
  };

  const handleUserSpeech = useMemoizedFn(async (transcript: string) => {
    if (!transcript.trim() || isProcessingRef.current) return;
    
    isProcessingRef.current = true;
    setIsLoading(true);
    
    console.log("User said:", transcript);
    
    const newHistory = [...chatHistory, { role: "user" as const, content: transcript }];
    setChatHistory(newHistory);
    
    const reply = await callChatAPI("chat", { 
      message: transcript, 
      history: chatHistory 
    });
    console.log("API reply:", reply);
    
    setChatHistory([...newHistory, { role: "assistant" as const, content: reply }]);
    
    await speakWithAvatar(reply);
    
    setIsLoading(false);
    isProcessingRef.current = false;
  });

  const startSession = useMemoizedFn(async () => {
    if (hasStartedRef.current) {
      console.log("Session already started, skipping...");
      return;
    }
    hasStartedRef.current = true;
    
    try {
      const newToken = await fetchAccessToken();
      const avatarInstance = initAvatar(newToken);

      avatarInstance.on(StreamingEvents.STREAM_READY, async (event) => {
        console.log(">>>>> Stream ready:", event.detail);
        
        if (!hasGreetedRef.current) {
          try {
            await new Promise(resolve => setTimeout(resolve, 1500));
            
            console.log("ğŸ”§ ì¸ì‚¬ë§ ìš”ì²­ ì¤‘...");
            console.log("ğŸ”§ í˜„ì¬ ì €ì¥ëœ userName:", userNameRef.current);
            console.log("ğŸ”§ í˜„ì¬ ì €ì¥ëœ stats:", userStatsRef.current);
            
            const greeting = await callChatAPI("greeting");
            console.log("ğŸ”§ ìƒì„±ëœ ì¸ì‚¬ë§:", greeting);

            await new Promise<void>((resolve) => {
              const onStopTalking = () => {
                console.log("ğŸ¤ ì•„ë°”íƒ€ ë§ ëë‚¨!");
                avatarInstance.off(StreamingEvents.AVATAR_STOP_TALKING, onStopTalking);
                resolve();
              };
              avatarInstance.on(StreamingEvents.AVATAR_STOP_TALKING, onStopTalking);
              speakWithAvatar(greeting);
            });

            setChatHistory([{ role: "assistant", content: greeting }]);
            console.log("Greeting sent successfully!");

            // ğŸ†• ë§ˆì´í¬ + VAD ì´ˆê¸°í™” (ì¸ì‚¬ í›„)
            await initMicAndVAD();
            console.log("ğŸ¤ ìŒì„± ì¸ì‹ ì¤€ë¹„ ì™„ë£Œ! ë§ì”€í•˜ì‹œë©´ ìë™ìœ¼ë¡œ ì¸ì‹í•©ë‹ˆë‹¤.");
            
            hasGreetedRef.current = true;
          } catch (error) {
            console.error("Error in greeting sequence:", error);
          }
        }
      });
      
      avatarInstance.on(StreamingEvents.STREAM_DISCONNECTED, () => {
        console.log("Stream disconnected");
        cleanupMicAndVAD();
        hasGreetedRef.current = false;
        hasStartedRef.current = false;
      });

      // ğŸ†• ì•„ë°”íƒ€ ë§í•˜ê¸° ì‹œì‘/ë ê°ì§€
      avatarInstance.on(StreamingEvents.AVATAR_START_TALKING, () => {
        console.log("ğŸ—£ï¸ ì•„ë°”íƒ€ ë§í•˜ê¸° ì‹œì‘ - VAD ì¼ì‹œ ì¤‘ì§€");
        isAvatarTalkingRef.current = true;
      });

      avatarInstance.on(StreamingEvents.AVATAR_STOP_TALKING, () => {
        console.log("ğŸ”‡ ì•„ë°”íƒ€ ë§í•˜ê¸° ë - VAD ì¬ê°œ");
        isAvatarTalkingRef.current = false;
      });

      await startAvatar(config);
      
    } catch (error) {
      console.error("Error starting avatar session:", error);
      hasStartedRef.current = false;
    }
  });

  const handleSendMessage = useMemoizedFn(async () => {
    const textToSend = inputText.trim();
    if (!textToSend || !avatarRef.current || isLoading) return;

    setInputText("");
    setIsLoading(true);

    const newHistory = [...chatHistory, { role: "user" as const, content: textToSend }];
    setChatHistory(newHistory);

    const reply = await callChatAPI("chat", {
      message: textToSend,
      history: chatHistory
    });

    setChatHistory([...newHistory, { role: "assistant" as const, content: reply }]);

    await speakWithAvatar(reply);

    setIsLoading(false);
  });

  const handleKeyPress = (e: React.KeyboardEvent) => {
    if (e.key === "Enter" && !e.shiftKey) {
      e.preventDefault();
      handleSendMessage();
    }
  };

  useUnmount(() => {
    stopAvatar();
    cleanupMicAndVAD();
    hasGreetedRef.current = false;
    hasStartedRef.current = false;
  });

  useEffect(() => {
    const handleMessage = async (event: MessageEvent) => {
      if (event.data && event.data.type === 'RESET_AVATAR') {
        console.log('ğŸ“¥ ì•„ë°”íƒ€ ë¦¬ì…‹ ì‹ í˜¸ ë°›ìŒ!');
        cleanupMicAndVAD();
        hasStartedRef.current = false;
        hasGreetedRef.current = false;
        userNameRef.current = '';
        userStatsRef.current = null;
        return;
      }
      
      if (event.data && event.data.type === 'START_AVATAR') {
        console.log('ğŸ“¥ ê²Œì„ì—ì„œ ì‹œì‘ ì‹ í˜¸ ë°›ìŒ!');
        console.log('ğŸ“¥ ë°›ì€ ë°ì´í„°:', event.data);
        console.log('ğŸ“¥ ì´ë¦„:', event.data.name);
        if (event.data.name) {
          userNameRef.current = event.data.name;
        }
        if (event.data.stats) {
          userStatsRef.current = event.data.stats;
          console.log('ğŸ“¥ stats ì €ì¥ë¨:', event.data.stats);
        }
        startSession();
      }
      
      if (event.data && event.data.type === 'EXPLAIN_GAME') {
        const game = event.data.game;
        console.log('ğŸ“¥ ê²Œì„ ì„¤ëª… ìš”ì²­:', game);
        
        if (avatarRef.current) {
          const explanation = await callChatAPI("game_explain", { game: game });
          console.log('ğŸ”§ ìƒì„±ëœ ê²Œì„ ì„¤ëª…:', explanation);
          speakWithAvatar(explanation);
        }
      }
    };
    
    window.addEventListener('message', handleMessage);
    return () => window.removeEventListener('message', handleMessage);
  }, []);

  useEffect(() => {
    if (stream && mediaStream.current) {
      mediaStream.current.srcObject = stream;
      mediaStream.current.onloadedmetadata = () => {
        mediaStream.current!.play();
      };
    }
  }, [mediaStream, stream]);

  return (
    <div className="w-full h-full flex flex-col">
      {sessionState === StreamingAvatarSessionState.CONNECTED && stream ? (
        <div className="flex-1 relative flex flex-col">
          <div className="relative flex-shrink-0">
            <video
              ref={mediaStream}
              autoPlay
              playsInline
              style={{ display: "block", width: "100%", height: "auto" }}
            />
            
            <button
              className="absolute top-2 right-2 w-7 h-7 bg-black/50 hover:bg-red-600 text-white rounded-full flex items-center justify-center text-xs transition-all"
              title="ì¢…ë£Œ"
              onClick={() => stopAvatar()}
            >
              âœ•
            </button>

            <div className="absolute bottom-2 left-2 flex items-center gap-2">
              <div className={`w-3 h-3 rounded-full ${
                isRecording ? 'bg-red-500 animate-pulse' : 
                isLoading ? 'bg-yellow-500' : 
                micEnabled ? 'bg-green-500' : 'bg-gray-500'
              }`} />
              <span className="text-white text-xs bg-black/50 px-2 py-1 rounded">
                {isRecording ? 'ğŸ¤ ë“£ëŠ” ì¤‘...' : 
                 isLoading ? 'ì‘ë‹µ ìƒì„± ì¤‘...' : 
                 micEnabled ? 'ë§ì”€í•˜ì„¸ìš”' : 'ë§ˆì´í¬ ì¤€ë¹„ ì¤‘...'}
              </span>
            </div>
          </div>

          <div className="p-2 bg-zinc-800 border-t border-zinc-700">
            <div className="flex gap-2">
              <input
                className="flex-1 px-3 py-2 bg-zinc-700 text-white text-sm rounded-lg border border-zinc-600 focus:outline-none focus:border-purple-500 disabled:opacity-50"
                disabled={isLoading}
                placeholder="ë˜ëŠ” í…ìŠ¤íŠ¸ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”..."
                type="text"
                value={inputText}
                onChange={(e) => setInputText(e.target.value)}
                onKeyPress={handleKeyPress}
              />
              <button
                className="px-4 py-2 bg-purple-600 hover:bg-purple-700 disabled:bg-zinc-600 text-white text-sm rounded-lg transition-colors"
                disabled={isLoading || !inputText.trim()}
                onClick={() => handleSendMessage()}
              >
                {isLoading ? "..." : "ì „ì†¡"}
              </button>
            </div>
          </div>
        </div>
      ) : (
        <div className="w-full h-full flex items-center justify-center bg-zinc-900">
          {sessionState === StreamingAvatarSessionState.CONNECTING ? (
            <div className="flex flex-col items-center gap-3 text-white">
              <div className="w-10 h-10 border-4 border-purple-500 border-t-transparent rounded-full animate-spin" />
              <span className="text-lg">ì—°ê²° ì¤‘...</span>
            </div>
          ) : (
            <div className="flex flex-col items-center gap-3 text-white">
              <span className="text-lg">ğŸ® ê²Œì„ì„ ì‹œì‘í•˜ë©´</span>
              <span className="text-lg">AI ë„ìš°ë¯¸ê°€ ë‚˜íƒ€ë‚˜ìš”!</span>
            </div>
          )}
        </div>
      )}
    </div>
  );
}

export default function InteractiveAvatarWrapper() {
  return (
    <StreamingAvatarProvider basePath={process.env.NEXT_PUBLIC_BASE_API_URL}>
      <InteractiveAvatar />
    </StreamingAvatarProvider>
  );
}
